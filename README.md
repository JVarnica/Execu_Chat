Execu_Chat is an on-device android app which runs LLMs using Executorch (wanted to keep Pytorch native, has kv_cache optimizations, and vulkan backend to use phone gpu). OFFLINE inference was the point- how well can a model run on a mobile phone and what size can it go up to?

##Features


